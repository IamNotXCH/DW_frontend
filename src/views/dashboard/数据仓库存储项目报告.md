# 数据仓库存储项目报告

[TOC]

## 爬虫部分概述

 在爬虫部分，我们使用了 scrapy 框架来实现。实现过程分为两个部分：1.针对正常网页进行爬虫；2.针对访问次数过多造成验证码拦截的页面进行处理。整个爬虫过程为：**快速大量爬虫 -> 针对无效数据，设置用户请求头和一定延迟再次爬虫 -> 仍受阻拦的少量数据通过调用接口破解验证码的方式进行第三次爬虫。**

### 正常爬虫

 在初次爬取网页时，我们首先尝试了如下的请求设置：

- **CONCURRENT_REQUESTS**: 允许最多 100 个并发请求；
- **DOWNLOAD_DELAY**: 每个请求之间引入 1 秒的延迟；
- **COOKIES_ENABLED**: 启用 cookies 进行会话管理；
- **DEFAULT_REQUEST_HEADERS**: 自定义请求头，模拟标准浏览器请求；
- **'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400**: 随机生成 User-Agent，降低被检测风险；
- **'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None**: 禁用默认 User-Agent 中间件。

 这种请求模式的优点是爬取速度极快，效率高，但是可能由于访问次数过多遭遇网站的拦截，形成大量的验证码阻拦。

 第二种请求设置加入了自动限速和重试的设置内容：

- **AUTOTHROTTLE_ENABLED**: 自动调整请求速率，以避免过载服务器。
- **AUTOTHROTTLE_START_DELAY**: 初始延迟为 1 秒。
- **AUTOTHROTTLE_MAX_DELAY**: 最大延迟为 10 秒。
- **AUTOTHROTTLE_TARGET_CONCURRENCY**: 目标为每秒 1 个请求。
- **RETRY_ENABLED**: 允许在请求失败时重试（最多 3 次）。

 这种请求设置的优点是能够避免很大一部分的验证码拦截，相比于直接处理验证码速度更快，但由于存在延迟、重试等，速度更慢，能够处理一部分上一次中爬取失败的数据。

#### CSV 处理

- 爬虫从 CSV 文件（`null_product_ids.csv`）中读取产品 ID。
- 每个产品 ID 用于构建指向产品页面的 URL。

#### 解析逻辑

- `parse` 方法检查 404 状态，并重试 CAPTCHA 挑战的页面。
- 使用 XPath 选择器从 Amazon 产品页面提取产品详细信息。
- 将详细信息存储在字典中，然后更新到 DataFrame。

#### 数据框更新

- `update_dataframe` 方法更新 DataFrame 中对应产品 ID 的详细信息。
- 如果遇到新的详细信息键，将创建一个新的列，并默认设置为 `'null'`。

#### 关闭爬虫

- 在关闭时，将更新后的 DataFrame 保存回 CSV 文件。

### 处理验证码拦截

 在爬取过程中，当访问速度过快或爬取次数过多，会触发验证码，导致无法爬取到有效内容，这种拦截即使在更改了爬虫请求设置后仍无法避免。我们的处理方式为：首先筛选出上一次爬虫中全为空的数据内容，单独成表，再次爬取。若遭遇验证码拦截，则使用 2Captcha API，识别验证码图片，并使用 Selenium WebDriver 填写验证码并提交，模拟通过验证码的过程。通过后进行正常的页面爬取，若遇到空产品则将 id 移除，防止出现多余的空行元素。

 由于采用这一方法时，Selenium WebDriver 开启、上传至 2Captcha 并等待 API 识别结果、页面跳转等等操作都很耗时，因此只在大量爬取遭遇拦截之后才使用。主要代码在`handleNullMovies.py`中。

#### handle_captcha

1. **打开页面**:
   - 使用 Selenium 打开包含 CAPTCHA 的产品页面。
2. **获取 CAPTCHA 图像**:
   - 使用 XPath 定位 CAPTCHA 图像，并获取其 URL。
3. **解决 CAPTCHA**:
   - 调用 `solve_captcha` 方法，将 CAPTCHA 图像传递给 2Captcha API 进行处理。
   - 如果成功得到解决方案，将其输入到 CAPTCHA 输入框中，并提交表单。
4. **等待页面加载**:
   - 使用 `WebDriverWait` 等待页面跳转到产品详情部分。
5. **获取页面内容**:
   - 获取更新后的页面源代码，并将其封装为 Scrapy 的 `Response` 对象，随后再次调用 `check_captcha` 方法。

#### solve_captcha

- **请求 CAPTCHA 图像**:
  - 使用 `requests` 库下载 CAPTCHA 图像，并保存为 `captcha.jpg`。
- **通过 2Captcha API 解决**:
  - 创建 `TwoCaptcha` 实例，调用 `normal` 方法提交 CAPTCHA 图像进行识别。
  - 如果成功，返回 CAPTCHA 的文本解决方案；如果失败，返回 `None`。

## 数据清洗

### 评论数据预处理

**1. 通过 ETL 对评论进行有效性判断**

**转换百分比**：将 helpfulness 从原始的分子/分母格式转换为百分比（即 (分子/分母) * 100）。

**有效性判断**：以 50% 为阈值，保留有效性大于 50% 的评论。认为这些评论具有一定的参考价值。

**数据处理逻辑清晰**：通过计算 helpfulness 的有效性比例，将原始数据进行筛选和清理，这一步主要目的是确保后续分析仅基于高质量数据，过滤掉低价值或噪声数据。

**2. 使用 NLP 库对评论进行情感分析**

在情感分析部分，您引入了自然语言处理（NLP）技术，对评论的情感倾向进行了分类：

**情感分类任务**：将评论的情感分为 **正面、负面和中性** 三种类型。为每条评论新增一个字段（如 ispositive），将正面情感标记为 1，负面或中性情感标记为 0。

**情感分析的意义**：通过分类评论的情感倾向，可以更好地了解评论数据的总体情绪分布，为产品优化、用户反馈分析等提供参考。



### ![Review](./Review.jpg)

### 爬取数据预处理

#### 1. 数据格式化
使用 `deal_processed.py` 脚本进行时长和日期的格式化，具体操作如下：
- **日期格式化**：将日期格式设置为 `xxxx/xx/xx`。
- **时长格式化**：将时长统一格式化为以分钟为单位的整数。

#### 2. 提取日期实体
- 对于合并后的电影数据表中的电影，使用 `date_generate.py` 脚本进行处理，将年份、月份、日期分别提取出并转换为整数类型，计算对应的季度（1-4）和星期信息（0-6），为每一日期赋予对应的唯一编号，并映射到原始的电影数据表中，映射方式为 `ASIN-date_id`。

#### 3. 提取电影实体
- 对于处理后的电影数据表进行 `movie_id` 的设置，为认定为同一部的电影赋予唯一的电影编号，并使用 `select_movie.py` 脚本进行电影信息提取，认定电影所有网页中最早的时间为电影的上映时间，所有网页的评论数目之和为电影的评论数目，所有评分的平均值为电影评分（保留两位小数）。

#### 4. 提取电影版本实体
- 对于处理后的电影数据表进行 `version_id` 的设置，使用 `movie_version.py` 脚本进行电影版本认定，认定方式如下：设置五个判定条件，分别为：
  - 年份相同或者一方为空
  - 日期相同或者一方为空
  - 月份相同或者一方为空
  - 载体形式相同或者一方为空
  - 时长相差小于 5 分钟或者一方为空
  
  满足五个判定条件则被认为是同一版本，赋予唯一的版本编号。为方便插入数据库，版本编号设置为全局递增。

- 使用 `select_version.py` 脚本进行电影版本信息的提取，在同一版本中进行日期的补全，并设置最全一列的 `date_id` 为 `releasetime_id`，并进行载体形式的补全和时长的统一，价格使用所有合并网页中的最低价作为该版本价格。

#### 5. 提取网页实体
- 使用 `version_web.py` 脚本进行网页映射，保留 `ASIN`、`URL` 以及 `name` 字段，以便进行溯源查询。

#### 6. 提取演员实体
- 对于处理后的演员合并表进行唯一编号设置，并通过 `ASIN-演员表` 进行 `actor_id` 到 `movie_id` 的映射。

#### 7. 提取导演实体
- 对于处理后的导演合并表进行唯一编号设置，并通过 `ASIN-导演表` 进行 `director_id` 到 `movie_id` 的映射。

#### 8. 提取类型实体
- 对于处理后的电影数据表，将 `type` 使用 `,` 分开作为值，提取去重后的 `type` 列赋予唯一编号，并设置 `movie_id` 到 `type_id` 的映射。

#### 9. 溯源数据
- 对于在数据处理过程中认定为非电影的数据，保存为 CSV 表格并进行 `ASIN` 去重处理，保留 `ASIN`、`URL`、`name` 字段，以便进行溯源查询。

#### 10. 评论关系提取
- 对处理后的评论数据表进行评论关系提取，完成 `comment_id` 到 `movie_id` 的映射。


## 数据仓库设计

### 基于MySQL的关系型数据库说明
   本项目的关系型数据库设计涵盖了电影、版本、类型、演员、导演和评论等信息。movie 表存储电影基本信息，movie_version 和 movie_version_detail 记录电影版本及其详细数据；movie_version_web 和 web 关联电影版本与商品网页；movie_type 和 type 管理电影与类型的关系；movie_actor 和 actor 处理电影与演员的关联；movie_director 和 director 关联导演与电影；review 表记录用户评论及评分。整个设计确保数据的全面性、灵活性，并支持高效查询与管理。

| 字段名称         | 字段类型 | 描述                                     |
| ---------------- | -------- | ---------------------------------------- |
| actor_id         | INT      | 外键，关联到 `actor` 表的演员 ID，表示演员的 ID |
| director_id      | INT      | 外键，关联到 `director` 表的导演 ID，表示导演的 ID |
| cooperation_count| INT      | 合作次数，表示该演员与导演合作的电影数量    |

#### 实体关系设计

##### 实体介绍
- **电影（Movie）**：主要实体，包含电影ID、名称、编剧、评分、评论数等基本信息。
- **版本（Movie Version）**：与电影相关的版本信息，包含版本ID、格式、时长、售价等。
- **类型（Movie Type）**：电影类型（如动作、喜剧等），每部电影可以有多个类型。
- **演员（Actor）**：表示电影中的演员，包含演员ID和演员姓名。
- **导演（Director）**：电影导演，包含导演ID和导演姓名。
- **评论（Review）**：记录电影的用户评论信息，包含评分、评论内容、有用性评分等。
- **日期（Date）**：记录日期信息，包括年份，月份，日期，季度等信息。
- **非电影（Drop Product）**：记录被筛选掉的网页信息，包括网页链接、ASIN、对应产品名称等。

##### 关系介绍
- **电影与版本**：电影与多个版本有一对多关系，通过movie_version 表进行关联。
- **版本与网页**：一个版本可能合并了多个网页，通过movie_version_web表进行关联
- **电影与演员**：电影与演员通过movie_actor 表建立多对多关系，一部电影可以有多个演员，演员可以参与多个电影。
- **电影与导演**：电影与导演是多对多关系，导演通过movie_director 表与电影关联。
- **电影与类型**：电影与类型通过movie_type 表进行多对多关联，表示每部电影可以有多个类型。
- **电影与评论**：电影和评论之间是一对多关系，一部电影可以有多个评论。
- **实体与日期**：电影、版本以及网页都与日期是多对一关系，在同一个日期可能有多个实体，但是一个实体只有一个日期
- **导演与演员**：导演与演员是多对多关系，一个导演可能和多个演员合作，一个演员也可能和多个导演合作，该关系有一个属性表示合作次数
- **演员与演员**：演员与演员是多对多关系，一个演员可能与多个演员合作过，该关系有一个属性为合作次数

#### 存储模型设计

##### E-R图

![关系型数据库ER图](./dsmdatabase.jpg)

##### 星型模型
星型模型（Star Schema） 是一种数据仓库设计中的常见模式，通常用于多维数据分析。星型模型由一个中央的**事实表（Fact Table）和多个维度表（Dimension Tables）**组成。事实表通常存储大量的数值数据（如销售额、评分等），而维度表则存储有关这些数据的描述性信息（如时间、地点、客户、产品等）。这种结构因为其直观、易于理解和高效的查询性能，广泛应用于数据仓库和业务智能（BI）系统中。

##### 选用原因

1. **高效查询电影信息**

    本项目需要频繁进行基于电影名称、评分、评论数等指标的分析。能够从星型模型的事实表中快速提取这些数据，维度表（如 actor、director、type、date）则提供了对这些数据的详细记录，支持高效的查询和分析。

2. **提供灵活查询选择**

    通过维度表（如 actor、director、type），可以根据不同的维度对电影数据进行灵活的切片和聚合。例如，可以分析某位导演合作的演员列表，或按电影类型统计评分等。

3. **易于扩展和维护：**

    随着数据量的增加（例如，更多的电影版本、更多的评论），星型模型易于扩展和维护。新维度和新数据可以通过添加新的维度表或事实表来实现，而不需要大规模重构现有数据库结构。

##### 主要事实表和维度表及作用
1. **主要事实表**
   - **movie**：核心的事实表，记录电影的关键度量数据，如评分、评论数等，支持对电影整体表现的分析。
   - **movie_version_detail**：该表记录与电影版本相关的具体信息，提供有关每个版本的详细度量数据（如售价、时长等），支持对不同版本的比较和分析。
   - **web**：该表记录与网页相关的具体信息，提供有关网页的数据，如ASIN以及URL，支持合并网页的溯源查询。
2. **主要维度表**
   - **actor**：为分析电影中参与的演员提供支持，可以通过该表进行演员维度的分析，探索演员与电影之间的关系。
   - **director**：提供导演相关的描述性数据，支持对导演执导的电影进行分析，并与电影的表现、演员合作等进行关联。
   - **type**：为电影的类型分类提供支持，通过该表可以分析电影的类型分布以及不同类型电影的表现。
   - **date**：为所有与时间相关的分析提供支持，可以基于日期维度进行如电影上映日期、评论日期等的时间序列分析。
   - **review**：为电影评论分析提供相应支持，可以基于电影评分、正面评价、有用性度量等进行特定电影的综合评论分析。

#### 关系型数据库物理模型

##### 基本表的DDL语句

```sql
CREATE TABLE IF NOT EXISTS dsmdatabase.actor
(
    actor_id   INT          NOT NULL
        PRIMARY KEY,
    actor_name VARCHAR(255) NOT NULL
);

CREATE INDEX actor_actor_name_index
    ON dsmdatabase.actor (actor_name);

CREATE TABLE IF NOT EXISTS dsmdatabase.date
(
    date_id INT NOT NULL
        PRIMARY KEY,
    year    INT NULL,
    month   INT NULL,
    day     INT NULL,
    season  INT NULL,
    weekday INT NULL
);

CREATE INDEX date_year_index
    ON dsmdatabase.date (year);

CREATE TABLE IF NOT EXISTS dsmdatabase.director
(
    director_id   INT          NOT NULL
        PRIMARY KEY,
    director_name VARCHAR(255) NOT NULL
);

CREATE INDEX director_director_name_index
    ON dsmdatabase.director (director_name);

CREATE TABLE IF NOT EXISTS dsmdatabase.drop_product
(
    ASIN VARCHAR(50)  NOT NULL
        PRIMARY KEY,
    url  VARCHAR(255) NOT NULL,
    name VARCHAR(255) NULL
);

CREATE TABLE IF NOT EXISTS dsmdatabase.movie
(
    movie_id          INT           NOT NULL
        PRIMARY KEY,
    date_id           INT           NOT NULL,
    name              VARCHAR(255)  NOT NULL,
    writer            VARCHAR(255)  NOT NULL,
    comment_count     INT           NOT NULL,
    grade             DECIMAL(3, 1) NOT NULL,
    good_review_count INT           NOT NULL,
    CONSTRAINT movie_date_date_id_fk
        FOREIGN KEY (date_id) REFERENCES dsmdatabase.date (date_id)
);

CREATE INDEX movie_name_index
    ON dsmdatabase.movie (name);

CREATE TABLE IF NOT EXISTS dsmdatabase.movie_actor
(
    actor_id INT NOT NULL,
    movie_id INT NOT NULL,
    PRIMARY KEY (movie_id, actor_id),
    CONSTRAINT movie_actor_actor_actor_id_fk
        FOREIGN KEY (actor_id) REFERENCES dsmdatabase.actor (actor_id),
    CONSTRAINT movie_actor_movie_movie_id_fk
        FOREIGN KEY (movie_id) REFERENCES dsmdatabase.movie (movie_id)
);

CREATE TABLE IF NOT EXISTS dsmdatabase.movie_director
(
    director_id INT NOT NULL,
    movie_id    INT NOT NULL,
    PRIMARY KEY (movie_id, director_id),
    CONSTRAINT movie_director_director_director_id_fk
        FOREIGN KEY (director_id) REFERENCES dsmdatabase.director (director_id),
    CONSTRAINT movie_director_movie_movie_id_fk
        FOREIGN KEY (movie_id) REFERENCES dsmdatabase.movie (movie_id)
);

CREATE TABLE IF NOT EXISTS dsmdatabase.movie_version_detail
(
    version_id     INT         NOT NULL
        PRIMARY KEY,
    releasetime_id INT         NOT NULL,
    format         VARCHAR(50) NULL,
    run_time       INT         NULL,
    cost           DECIMAL     NULL,
    CONSTRAINT movie_version_detail_date_date_id_fk
        FOREIGN KEY (releasetime_id) REFERENCES dsmdatabase.date (date_id)
);

CREATE TABLE IF NOT EXISTS dsmdatabase.movie_version
(
    movie_id   INT NOT NULL,
    version_id INT NOT NULL,
    PRIMARY KEY (version_id, movie_id),
    CONSTRAINT movie_version_movie_movie_id_fk
        FOREIGN KEY (movie_id) REFERENCES dsmdatabase.movie (movie_id),
    CONSTRAINT movie_version_movie_version_detail_version_id_fk
        FOREIGN KEY (version_id) REFERENCES dsmdatabase.movie_version_detail (version_id)
);

CREATE TABLE IF NOT EXISTS dsmdatabase.review
(
    helpfulness DECIMAL(5, 2) NOT NULL,
    score       INT           NOT NULL,
    is_positive TINYINT(1)    NOT NULL,
    comment_id  INT AUTO_INCREMENT
        PRIMARY KEY,
    movie_id    INT           NOT NULL,
    CONSTRAINT review_movie_movie_id_fk
        FOREIGN KEY (movie_id) REFERENCES dsmdatabase.movie (movie_id)
);

CREATE TABLE IF NOT EXISTS dsmdatabase.type
(
    type_id   INT         NOT NULL
        PRIMARY KEY,
    type_name VARCHAR(50) NOT NULL
);

CREATE TABLE IF NOT EXISTS dsmdatabase.movie_type
(
    movie_id INT NOT NULL,
    type_id  INT NOT NULL,
    PRIMARY KEY (movie_id, type_id),
    CONSTRAINT movie_type_movie_movie_id_fk
        FOREIGN KEY (movie_id) REFERENCES dsmdatabase.movie (movie_id),
    CONSTRAINT movie_type_type_type_id_fk
        FOREIGN KEY (type_id) REFERENCES dsmdatabase.type (type_id)
);

CREATE TABLE IF NOT EXISTS dsmdatabase.web
(
    ASIN    VARCHAR(50)  NOT NULL
        PRIMARY KEY,
    url     VARCHAR(255) NOT NULL,
    date_id INT          NOT NULL,
    CONSTRAINT web_date_date_id_fk
        FOREIGN KEY (date_id) REFERENCES dsmdatabase.date (date_id)
);

CREATE TABLE IF NOT EXISTS dsmdatabase.movie_version_web
(
    version_id INT         NOT NULL,
    ASIN       VARCHAR(50) NOT NULL,
    PRIMARY KEY (version_id, ASIN),
    CONSTRAINT movie_version_web_movie_version_detail_version_id_fk
        FOREIGN KEY (version_id) REFERENCES dsmdatabase.movie_version_detail (version_id),
    CONSTRAINT movie_version_web_web_ASIN_fk
        FOREIGN KEY (ASIN) REFERENCES dsmdatabase.web (ASIN)
);

```



### 基于Hive的分布式存储说明

#### hadoop集群配置

本项目中，我们采用了hive作为分布式文件系统存储模型。Hive适合处理大规模数据集的批量查询和分析，特别是在不需要即时响应的场景下表现优异。针对大规模数据的聚合统计、报表生成和复杂的多表连接查询，Hive能够高效地完成任务。

项目配置了4台阿里云服务器，分别作为hadoop存储系统的1台master机器和3台slave机器。4台机器间通过ssh配置进行免密连接，并配置了hdfs集群以及yarn集群进行任务调度，并在主机master配置了hive用于整体的建表、hiveserver2的运行、命令调度等。集群规划如下：

| 服务器IP        | 106.15.36.155 | 106.14.222.122 | 47.100.162.36 | 106.14.44.200 |
| --------------- | ------------- | -------------- | ------------- | ------------- |
| 主机名          | master        | slave1         | slave2        | slave3        |
| NameNode        | 是            |                |               |               |
| Secondary       |               | 是             |               |               |
| DataNode        | 是            | 是             | 是            | 是            |
| ResourceManager | 是            |                |               |               |
| NodeManager     | 是            | 是             | 是            | 是            |

#### **存储设计：**

由于选择了mySQL作为hive的元数据，因此表定义与关系型存储方式中的表定义相同，在存储方式上仅使用mySQL存储表格的元数据，其余数据存储在hive表中。如：

![hive-1](.\hive-1.png)

### 基于Neo4j的图数据存储说明

#### 概览图

![总图概览](overall.png)

#### 数据存储设计模型

在我们的项目中，基于Neo4j图数据库的设计模型非常适合用于处理复杂的关系数据。根据项目的需求，我们设计了以下六个主要节点类型和相关的关系：

**节点类型：**
- **Movie (电影节点)**：存储电影的基本信息（名称、演员、导演、风格、作家）。
- **Actor (演员节点)**：存储演员的信息（演员ID、姓名）。
- **Director (导演节点)**：存储导演的信息（导演ID、姓名）。
- **Review (评论节点)**：存储评论用户评论的信息（评论有用度、评分、评价是否积极）。
- **Version (版本节点)**：存储电影的不同版本（版本号、版本评论数、版本格式、版本评分。上映日期、时长）。
- **Type (电影类别节点)**：存储电影的类型（如Action、Adventure、Drama等）。

**关系类型：**
- **HAS_VERSION**：表示电影和其不同版本之间的关系。
- **HAS_ACTOR**：表示演员和电影之间的关系。
- **HAS_DIRECTOR**：表示导演和电影之间的关系。
- **BELONGS_TO_TYPE**：表示电影和其类型之间的关系。
- **IS_REVIEWED_BY**：表示电影和用户评论之间的关系。
- **COOPERATES_WITH**：表示演员和导演之间的合作关系。

**关系图示例如下：**
![图数据库存储设计](graph.png)

#### **关于初始数据导入**

在最初的数据导入尝试中，由于直接在服务器上执行导入操作，受限于服务器的硬件性能和资源（如 CPU、内存、磁盘 I/O 等）的瓶颈，在面对大规模数据处理时，服务器经常出现响应缓慢甚至卡死的情况。为了克服这些问题，我们调整了数据导入策略，采用了 **“本地构建、远程迁移”** 的方案：

1. **本地构建图数据库**：我们首先在本地机器上完成数据的清洗、导入和图数据库的构建工作。本地环境通常资源更充足，能够快速完成节点和关系的创建。同时，图数据库的所有索引、模式约束等也在本地环境中预先设置完成。

2. **使用 `.dump` 文件迁移**：本地构建完成后，利用 Neo4j 提供的备份和恢复工具，生成完整的数据库快照文件（`.dump` 文件），并通过安全的文件传输协议（ `SCP`）将 `.dump` 文件上传到目标服务器。在目标服务器上，我们使用 `neo4j-admin` 工具加载该文件，从而实现数据库的迁移。

   ```shell
   //本地
   neo4j-admin database dump --to-path=./backup amazon.movie.data
   // 传输
   scp 
   \ .../neo4j-community-5.25.1-windows/neo4j-community-5.25.1/backup/amazon.moive.data.dump 
   \ root@139.224.137.93:/var/lib/neo4j/data/dumps
   // 恢复数据库
   sudo neo4j-admin database load 
   \ --from-path=/var/lib/neo4j/data/dumps 
   \ --overwrite-destination=true amazon.movie.data
   
   ```
3. **分批导入策略**：对于数据量较大的场景，为了避免一次性导入大量数据导致服务器缓存不足或内存溢出，我们采用了分批导入的方式。例如，使用 Cypher 中的 `WITH row SKIP X LIMIT Y` 子句，通过指定跳过一定数量的记录并限制每次导入的数量，从而控制数据导入的批次和规模。这种策略不仅减轻了硬件的压力，同时也增强了导入过程的可控性和稳定性。


通过这些优化措施，我们有效解决了因硬件性能不足引发的导入问题，成功实现了大规模数据的高效迁移与加载。

## 存储方式与优化

### 关系型数据库（MySQL）

- **适用查询**：关系型数据库在复杂的多表关联查询、数据筛选与过滤、聚合分析查询、排序与分页查询、复杂条件逻辑查询、事务处理相关查询以及层次化查询等场景中都较为适用。通过 JOIN 实现跨表查询，利用条件筛选和聚合函数进行数据分析，可以满足不同场景下的数据提取和分析需求。此外，SQL 还支持复杂的嵌套查询、排序分页以及事务操作，确保数据的一致性和完整性，适用于如电影数据库查询、订单处理等多种应用场景。
- **优化工作**：
  - **频繁查询字段建立索引**：为常用查询字段（如导演、演员、电影名称等）创建索引，提高查询效率。  
  - **反范式化（Denormalization）**：  
    - 冗余存储电影的评价信息（如 `good_review_count`），避免每次查询时从 `review` 表中进行统计计算。  
    - 冗余存储 `season` 和 `weekday` 信息，减少查询时的计算开销。  
    - 创建 `actor_director_cooperation` 表记录演员与导演的合作次数，避免多表联结。  
    - 创建 `actor_actor_cooperation` 表记录演员之间的合作次数，避免重复计算。  
- **比较结果**：优化后的查询显著提高了性能。在“获取最多好评的电影”查询中，响应时间从6秒减少至78毫秒；在“季度电影查询”中，通过冗余存储季度信息，查询时间稳定在30毫秒以内；而在“演员合作查询”中，预存合作次数将查询时间从接近4秒减少至200-300毫秒，显著提升了处理效率，尤其在频繁查询时效果突出。

### 分布式文件系统（Hive）

- **适用查询**：Hive适合处理大规模数据集的批量查询和分析，特别是不需要即时响应的场景。
- **优化工作**：
  - **数据分区和桶化**：根据电影的发行年份、类型等字段对数据进行分区和桶化，提高特定查询的效率。
  - **文件格式选择**：采用如Parquet等列式存储格式，以优化读取效率和压缩数据大小。
  - **计算模型Spark**：采用Spark计算框架模型，使查询效率更高。
- **比较结果**：优化后的Hive环境在处理大批量数据时速度提升明显，尤其在执行复杂的数据汇总和分析查询时效率提高了约30%。

### 图数据库（Neo4j）

- **适用查询**：Neo4j适合处理复杂的关系和图形查询，例如分析演员和导演之间的合作网络。
- **优化工作**：
  - **对于版本节点**：在数据库中，不同版本节点如果具有相同的 **`Time`**、**`ReleaseTime`** 和 **`Format`**，可以认为它们是同一个版本。为优化存储和查询效率，我们可以通过以下方法进行处理和优化：增加 `Version` 属性，实现唯一版本标识，统计电影的实际版本数量。
  - **索引和约束**：对关键属性（如演员名、导演名）建立索引，以快速定位节点和关系。
- **比较结果**：优化后，在执行涉及复杂关系的查询时，我们观察到平均查询时间减少了约50%，特别是在分析演员和导演之间的合作模式时。

## **关于数据治理体系**

为了确保数据在整个生命周期中的高质量、可追溯性和合规性，我们建立了一套系统化的数据治理体系，该体系主要包含以下几个关键维度：

1. **数据质量管理**：
   - 在数据质量方面，我们使用了标准化的 **ETL（Extract, Transform, Load）** 流程对原始数据进行清洗、转换和加载操作。
   - **数据完整性**：清洗过程中，我们确保了所有必要字段均被填充且数据类型一致。例如，在 `Version` 节点中，`versionId` 和 `ReleaseTime` 属性必须存在并满足格式要求。
   - **数据一致性**：对多源数据进行规范化处理，确保同一实体（如电影名称或版本号）在不同来源间具有一致的表达方式。
   - **数据准确性**：通过规则校验和异常检测，剔除数据中可能的错误值或重复值。例如，对于评论的评分字段（`score`），确保其数值在规定范围内（如 `0-5`）。


2. **数据溯源查询**：
   - 数据溯源是图数据库的核心优势之一，我们通过路径查询功能实现了数据来源的清晰追踪。
       - **节点关系追踪**：通过查询如 `MATCH (m:Movie)-[:HAS_VERSION]->(v:Version)` 的关系结构，可以从具体的版本数据回溯到其对应的电影信息，进一步关联到数据的源头。
       - **多跳路径分析**：对于涉及多级数据关联的场景（如用户评论、电影版本、电影类型的层级关系），我们可以通过多跳路径查询（`MATCH p = (m:Movie)-[:HAS_VERSION*1..3]->(v:Version)`）清晰描绘数据之间的依赖关系，确保数据来源的准确性和可验证性。
       - **数据变更记录**：结合 Neo4j 的审计和日志功能，记录数据的插入、更新和删除操作，保证数据变更可被追踪。
   - 关系型数据库通常使用表格结构来存储数据，并通过外键和索引来建立表之间的关系。
       - **表关联追踪**：在关系型数据库中，通过设计合理的外键约束，可以建立表之间的关系。例如，电影信息表和版本表之间可能通过电影 ID 进行关联，类似于图数据库中的节点关系。可以使用 SQL 查询如 `SELECT * FROM movie m JOIN version v ON m.movie_id = v.movie_id` 来追踪数据源。
       - **多表联合查询**：对于多级数据关联的场景，关系型数据库通过多表联合查询来分析数据之间的关系。例如，查询用户评论、电影版本和网页之间的层级关系，需要使用多表连接来查询。
       - **事务日志与数据追踪**：在关系型数据库中，事务日志可以用于追踪数据的变动和表之间的关系。每当数据被修改时，MySQL 会将这些操作记录在二进制日志中，使数据库能够回溯并重放这些操作，追踪数据的历史变更，开发者可以通过分析日志来追踪数据变化的源头。
3. **数据合规性与权限控制**：
   - **数据合规性**：我们在数据导入和迁移过程中严格遵循数据使用和隐私保护的相关规定，确保数据符合 GDPR 等隐私保护法规。
   - **权限分级管理**：
     - 通过图数据库的内置权限机制（如用户角色和访问控制），我们对不同角色（如开发人员、分析人员和管理员）的数据访问权限进行了明确的分级，避免了越权访问或数据泄露的风险
     - 通过 MySQL 的权限控制机制，设置不同角色的访问权限，确保不同用户（如开发人员、分析人员、管理员）只能访问其授权的数据。使用 GRANT 和 REVOKE 命令进行权限管理，支持细粒度权限控制（如基于列的权限），并通过审计日志确保数据访问的透明性和合规性。
4. **性能优化与监控**：
   - **索引优化**：针对高频查询的数据（如 `Movie` 的 `name` 属性、`Version` 的 `versionId` 属性等），我们创建了相应的索引，提高了查询性能。
   - **资源监控**：借助 Neo4j、MySQL 提供的监控工具，我们对数据导入和查询过程中的资源使用情况（如内存、CPU 和 I/O）进行了实时监控，确保系统稳定运行。
   - **物理存储结构优化**：我们通过创建额外的关联表（如演员与演员合作表）来优化查询效率。这些表有助于将关联数据进行物理分离，减少多表联合查询时的负担。例如，在查询某两个演员的合作次数时，通过预先计算并存储在单独的表中，能够大幅度提高查询的响应速度。

## 数据血缘

在本项目中，我们采取了一系列措施来确保数据血缘的完整性和准确性，以保障数据流动的可追溯性和透明度。这些措施不仅帮助我们有效管理数据源、版本和合并过程，还为数据质量控制和问题定位提供了支持。

### 项目中的数据血缘记录

1. **元数据的管理**：
   - 我们对每个数据项的元数据进行了详细管理，例如电影的ASIN、URL、日期等信息。这些元数据帮助我们清楚地了解每个数据项在数据流中的位置，并确保其来源与变动路径可以追溯。

2. **数据合并过程的追踪**：
   - 在网页合并过程中，我们记录了每个网页的合并步骤及其与电影版本的对应关系。例如，在处理哈利波特第一部电影时，我们详细记录了每个合并的网页信息，确保合并过程中的每一步都可以追溯。

3. **中间数据结果的保存与验证**：  
   - 在数据处理的每个关键步骤，我们都会保存中间结果。通过记录这些中间数据，我们能够验证每个处理环节的有效性，确保最终结果的准确性。此外，这些中间数据也为后续的数据调整和重处理提供了参考。

### 项目中的数据血缘应用场景

在本项目中，数据血缘主要应用于以下几个方面：

1. **追踪电影数据源**：
   - 数据血缘帮助我们追踪每部电影的数据来源及其变动历史，在处理哈利波特系列电影时，能够清楚地知道每个电影版本的网页合并过程和版本信息。

2. **网页数据合并与处理**：
   - 在多个网页的数据合并过程中，数据血缘确保了每个网页的合并步骤得到详细记录，保证了每个版本的数据是通过正确的网页合并生成的。

3. **版本控制与数据验证**：
   - 我们使用数据血缘来控制和验证不同版本的数据，确保每个版本的生成过程清晰可追溯，避免数据合并时出现错误或数据丢失。

### 广义场景下的数据血缘应用

1. **数据质量管理**：
   - 数据血缘能够帮助我们监控数据的质量，确保数据的准确性和完整性。通过追溯数据源和处理过程，能够及时发现和解决数据质量问题，提升数据的可靠性。

2. **合规性与审计**：
   - 数据血缘在合规性和审计方面至关重要。它能够清楚地展示数据的处理流程和来源，确保数据符合相关法规要求，便于后续的审计和合规检查。

3. **数据变更追踪与问题定位**：
   - 数据血缘帮助我们追踪数据在处理过程中的变动，能够快速定位数据变更的原因及其影响。例如，在数据出错时，通过血缘追踪可以迅速找到数据源头和出错环节，从而加快问题修复速度。

4. **数据共享与协作**：
   - 在跨团队的数据共享和协作中，数据血缘能够提供清晰的数据流动视图，帮助不同团队成员理解数据的来源和变更，促进数据的有效共享和协作。




## 使用到的软件的版本

- Hadoop：3.4.0

- Hive：3.1.13

- Neo4j：5.25.1

- MySQL：8.0.36